{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neuralstylefinalfinal.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "UIuzBn42tBD8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "508e94d9-6b44-4cc4-f847-8f8961a68b36",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519406821149,
          "user_tz": -330,
          "elapsed": 3101,
          "user": {
            "displayName": "Radhika Dua",
            "photoUrl": "//lh5.googleusercontent.com/-cNc0wW8Lckg/AAAAAAAAAAI/AAAAAAAAG5U/08TmYFVbosY/s50-c-k-no/photo.jpg",
            "userId": "105189358300007094605"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/llSourcell/How-to-Generate-Art-Demo.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'How-to-Generate-Art-Demo' already exists and is not an empty directory.\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wDQo7s5dtCIO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aec36677-9dad-41a1-bc94-41eef81b036c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519406845419,
          "user_tz": -330,
          "elapsed": 820,
          "user": {
            "displayName": "Radhika Dua",
            "photoUrl": "//lh5.googleusercontent.com/-cNc0wW8Lckg/AAAAAAAAAAI/AAAAAAAAG5U/08TmYFVbosY/s50-c-k-no/photo.jpg",
            "userId": "105189358300007094605"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "Image('images/15_style_transfer_flowchart.png')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "images/15_style_transfer_flowchart.png",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "qGywvKf-tIk4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import PIL.Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4GgqjQVQtKl0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77c4d7e3-3ef2-4d53-d90f-1b2b84315056",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519406864443,
          "user_tz": -330,
          "elapsed": 786,
          "user": {
            "displayName": "Radhika Dua",
            "photoUrl": "//lh5.googleusercontent.com/-cNc0wW8Lckg/AAAAAAAAAAI/AAAAAAAAG5U/08TmYFVbosY/s50-c-k-no/photo.jpg",
            "userId": "105189358300007094605"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.6.0-rc1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "czoywVyctNPa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "########################################################################\n",
        "#\n",
        "# Functions for downloading and extracting data-files from the internet.\n",
        "#\n",
        "# Implemented in Python 3.5\n",
        "#\n",
        "########################################################################\n",
        "#\n",
        "# This file is part of the TensorFlow Tutorials available at:\n",
        "#\n",
        "# https://github.com/Hvass-Labs/TensorFlow-Tutorials\n",
        "#\n",
        "# Published under the MIT License. See the file LICENSE for details.\n",
        "#\n",
        "# Copyright 2016 by Magnus Erik Hvass Pedersen\n",
        "#\n",
        "########################################################################\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import zipfile\n",
        "\n",
        "########################################################################\n",
        "\n",
        "\n",
        "def _print_download_progress(count, block_size, total_size):\n",
        "    \"\"\"\n",
        "    Function used for printing the download progress.\n",
        "    Used as a call-back function in maybe_download_and_extract().\n",
        "    \"\"\"\n",
        "\n",
        "    # Percentage completion.\n",
        "    pct_complete = float(count * block_size) / total_size\n",
        "\n",
        "    # Status-message. Note the \\r which means the line should overwrite itself.\n",
        "    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
        "\n",
        "    # Print it.\n",
        "    sys.stdout.write(msg)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "########################################################################\n",
        "\n",
        "\n",
        "def maybe_download_and_extract(url, download_dir):\n",
        "    \"\"\"\n",
        "    Download and extract the data if it doesn't already exist.\n",
        "    Assumes the url is a tar-ball file.\n",
        "\n",
        "    :param url:\n",
        "        Internet URL for the tar-file to download.\n",
        "        Example: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "\n",
        "    :param download_dir:\n",
        "        Directory where the downloaded file is saved.\n",
        "        Example: \"data/CIFAR-10/\"\n",
        "\n",
        "    :return:\n",
        "        Nothing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Filename for saving the file downloaded from the internet.\n",
        "    # Use the filename from the URL and add it to the download_dir.\n",
        "    filename = url.split('/')[-1]\n",
        "    file_path = os.path.join(download_dir, filename)\n",
        "\n",
        "    # Check if the file already exists.\n",
        "    # If it exists then we assume it has also been extracted,\n",
        "    # otherwise we need to download and extract it now.\n",
        "    if not os.path.exists(file_path):\n",
        "        # Check if the download directory exists, otherwise create it.\n",
        "        if not os.path.exists(download_dir):\n",
        "            os.makedirs(download_dir)\n",
        "\n",
        "        # Download the file from the internet.\n",
        "        file_path, _ = urllib.request.urlretrieve(url=url,\n",
        "                                                  filename=file_path,\n",
        "                                                  reporthook=_print_download_progress)\n",
        "\n",
        "        print()\n",
        "        print(\"Download finished. Extracting files.\")\n",
        "\n",
        "        if file_path.endswith(\".zip\"):\n",
        "            # Unpack the zip-file.\n",
        "            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n",
        "        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
        "            # Unpack the tar-ball.\n",
        "            tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n",
        "\n",
        "        print(\"Done.\")\n",
        "    else:\n",
        "        print(\"Data has apparently already been downloaded and unpacked.\")\n",
        "\n",
        "\n",
        "########################################################################\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rQqG7jkvtQZQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# The pre-trained VGG16 Model for TensorFlow.\n",
        "#\n",
        "# This model seems to produce better-looking images in Style Transfer\n",
        "# than the Inception 5h model that otherwise works well for DeepDream.\n",
        "#\n",
        "# Implemented in Python 3.5 with TensorFlow v0.12.0rc1\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# The pre-trained VGG16 model is taken from this tutorial:\n",
        "# https://github.com/pkmital/CADL/blob/master/session-4/libs/vgg16.py\n",
        "\n",
        "# The class-names are available in the following URL:\n",
        "# https://s3.amazonaws.com/cadl/models/synset.txt\n",
        "\n",
        "# Internet URL for the file with the VGG16 model.\n",
        "# Note that this might change in the future and will need to be updated.\n",
        "data_url = \"https://s3.amazonaws.com/cadl/models/vgg16.tfmodel\"\n",
        "\n",
        "# Directory to store the downloaded data.\n",
        "data_dir = \"vgg16/\"\n",
        "\n",
        "# File containing the TensorFlow graph definition. (Downloaded)\n",
        "path_graph_def = \"vgg16.tfmodel\"\n",
        "\n",
        "\n",
        "\n",
        "def maybe_download():\n",
        "    \"\"\"\n",
        "    Download the VGG16 model from the internet if it does not already\n",
        "    exist in the data_dir. The file is about 550 MB.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Downloading VGG16 Model ...\")\n",
        "\n",
        "    # The file on the internet is not stored in a compressed format.\n",
        "    # This function should not extract the file when it does not have\n",
        "    # a relevant filename-extensions such as .zip or .tar.gz\n",
        "    maybe_download_and_extract(url=data_url, download_dir=data_dir)\n",
        "\n",
        "\n",
        "class VGG16:\n",
        "    \"\"\"\n",
        "    The VGG16 model is a Deep Neural Network which has already been\n",
        "    trained for classifying images into 1000 different categories.\n",
        "\n",
        "    When you create a new instance of this class, the VGG16 model\n",
        "    will be loaded and can be used immediately without training.\n",
        "    \"\"\"\n",
        "\n",
        "    # Name of the tensor for feeding the input image.\n",
        "    tensor_name_input_image = \"images:0\"\n",
        "\n",
        "    # Names of the tensors for the dropout random-values..\n",
        "    tensor_name_dropout = 'dropout/random_uniform:0'\n",
        "    tensor_name_dropout1 = 'dropout_1/random_uniform:0'\n",
        "\n",
        "    # Names for the convolutional layers in the model for use in Style Transfer.\n",
        "    layer_names = ['conv1_1/conv1_1', 'conv1_2/conv1_2',\n",
        "                   'conv2_1/conv2_1', 'conv2_2/conv2_2',\n",
        "                   'conv3_1/conv3_1', 'conv3_2/conv3_2', 'conv3_3/conv3_3',\n",
        "                   'conv4_1/conv4_1', 'conv4_2/conv4_2', 'conv4_3/conv4_3',\n",
        "                   'conv5_1/conv5_1', 'conv5_2/conv5_2', 'conv5_3/conv5_3']\n",
        "\n",
        "    def __init__(self):\n",
        "        # Now load the model from file. The way TensorFlow\n",
        "        # does this is confusing and requires several steps.\n",
        "\n",
        "        # Create a new TensorFlow computational graph.\n",
        "        self.graph = tf.Graph()\n",
        "\n",
        "        # Set the new graph as the default.\n",
        "        with self.graph.as_default():\n",
        "\n",
        "            # TensorFlow graphs are saved to disk as so-called Protocol Buffers\n",
        "            # aka. proto-bufs which is a file-format that works on multiple\n",
        "            # platforms. In this case it is saved as a binary file.\n",
        "\n",
        "            # Open the graph-def file for binary reading.\n",
        "            path = os.path.join(data_dir, path_graph_def)\n",
        "            with tf.gfile.FastGFile(path, 'rb') as file:\n",
        "                # The graph-def is a saved copy of a TensorFlow graph.\n",
        "                # First we need to create an empty graph-def.\n",
        "                graph_def = tf.GraphDef()\n",
        "\n",
        "                # Then we load the proto-buf file into the graph-def.\n",
        "                graph_def.ParseFromString(file.read())\n",
        "\n",
        "                # Finally we import the graph-def to the default TensorFlow graph.\n",
        "                tf.import_graph_def(graph_def, name='')\n",
        "\n",
        "                # Now self.graph holds the VGG16 model from the proto-buf file.\n",
        "\n",
        "            # Get a reference to the tensor for inputting images to the graph.\n",
        "            self.input = self.graph.get_tensor_by_name(self.tensor_name_input_image)\n",
        "\n",
        "            # Get references to the tensors for the commonly used layers.\n",
        "            self.layer_tensors = [self.graph.get_tensor_by_name(name + \":0\") for name in self.layer_names]\n",
        "\n",
        "    def get_layer_tensors(self, layer_ids):\n",
        "        \"\"\"\n",
        "        Return a list of references to the tensors for the layers with the given id's.\n",
        "        \"\"\"\n",
        "\n",
        "        return [self.layer_tensors[idx] for idx in layer_ids]\n",
        "\n",
        "    def get_layer_names(self, layer_ids):\n",
        "        \"\"\"\n",
        "        Return a list of names for the layers with the given id's.\n",
        "        \"\"\"\n",
        "\n",
        "        return [self.layer_names[idx] for idx in layer_ids]\n",
        "\n",
        "    def get_all_layer_names(self, startswith=None):\n",
        "        \"\"\"\n",
        "        Return a list of all the layers (operations) in the graph.\n",
        "        The list can be filtered for names that start with the given string.\n",
        "        \"\"\"\n",
        "\n",
        "        # Get a list of the names for all layers (operations) in the graph.\n",
        "        names = [op.name for op in self.graph.get_operations()]\n",
        "\n",
        "        # Filter the list of names so we only get those starting with\n",
        "        # the given string.\n",
        "        if startswith is not None:\n",
        "            names = [name for name in names if name.startswith(startswith)]\n",
        "\n",
        "        return names\n",
        "\n",
        "    def create_feed_dict(self, image):\n",
        "        \"\"\"\n",
        "        Create and return a feed-dict with an image.\n",
        "\n",
        "        :param image:\n",
        "            The input image is a 3-dim array which is already decoded.\n",
        "            The pixels MUST be values between 0 and 255 (float or int).\n",
        "\n",
        "        :return:\n",
        "            Dict for feeding to the graph in TensorFlow.\n",
        "        \"\"\"\n",
        "\n",
        "        # Expand 3-dim array to 4-dim by prepending an 'empty' dimension.\n",
        "        # This is because we are only feeding a single image, but the\n",
        "        # VGG16 model was built to take multiple images as input.\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "\n",
        "        if False:\n",
        "            # In the original code using this VGG16 model, the random values\n",
        "            # for the dropout are fixed to 1.0.\n",
        "            # Experiments suggest that it does not seem to matter for\n",
        "            # Style Transfer, and this causes an error with a GPU.\n",
        "            dropout_fix = 1.0\n",
        "\n",
        "            # Create feed-dict for inputting data to TensorFlow.\n",
        "            feed_dict = {self.tensor_name_input_image: image,\n",
        "                         self.tensor_name_dropout: [[dropout_fix]],\n",
        "                         self.tensor_name_dropout1: [[dropout_fix]]}\n",
        "        else:\n",
        "            # Create feed-dict for inputting data to TensorFlow.\n",
        "            feed_dict = {self.tensor_name_input_image: image}\n",
        "\n",
        "        return feed_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ON6FC6yEtY75",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import vgg16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yw41vnImtddN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# vgg16.data_dir = 'vgg16/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8HZwaIB0tlFf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1da5596b-beca-471d-9538-c3b8c2eb86e6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519406989051,
          "user_tz": -330,
          "elapsed": 763,
          "user": {
            "displayName": "Radhika Dua",
            "photoUrl": "//lh5.googleusercontent.com/-cNc0wW8Lckg/AAAAAAAAAAI/AAAAAAAAG5U/08TmYFVbosY/s50-c-k-no/photo.jpg",
            "userId": "105189358300007094605"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "maybe_download()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading VGG16 Model ...\n",
            "Data has apparently already been downloaded and unpacked.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PR4_39PutrqB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def load_image(filename, max_size=None):\n",
        "    image = PIL.Image.open(filename)\n",
        "\n",
        "    if max_size is not None:\n",
        "        # Calculate the appropriate rescale-factor for\n",
        "        # ensuring a max height and width, while keeping\n",
        "        # the proportion between them.\n",
        "        factor = max_size / np.max(image.size)\n",
        "    \n",
        "        # Scale the image's height and width.\n",
        "        size = np.array(image.size) * factor\n",
        "\n",
        "        # The size is now floating-point because it was scaled.\n",
        "        # But PIL requires the size to be integers.\n",
        "        size = size.astype(int)\n",
        "\n",
        "        # Resize the image.\n",
        "        image = image.resize(size, PIL.Image.LANCZOS)\n",
        "\n",
        "    # Convert to numpy floating-point array.\n",
        "    return np.float32(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7tPttBZatvVJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def save_image(image, filename):\n",
        "    # Ensure the pixel-values are between 0 and 255.\n",
        "    image = np.clip(image, 0.0, 255.0)\n",
        "    \n",
        "    # Convert to bytes.\n",
        "    image = image.astype(np.uint8)\n",
        "    \n",
        "    # Write the image-file in jpeg-format.\n",
        "    with open(filename, 'wb') as file:\n",
        "        PIL.Image.fromarray(image).save(file, 'jpeg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "meDLxC-etyqM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def plot_image_big(image):\n",
        "    # Ensure the pixel-values are between 0 and 255.\n",
        "    image = np.clip(image, 0.0, 255.0)\n",
        "\n",
        "    # Convert pixels to bytes.\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    # Convert to a PIL-image and display it.\n",
        "    display(PIL.Image.fromarray(image))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "paMz3Kctt1i7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def plot_images(content_image, style_image, mixed_image):\n",
        "    # Create figure with sub-plots.\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(10, 10))\n",
        "\n",
        "    # Adjust vertical spacing.\n",
        "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
        "\n",
        "    # Use interpolation to smooth pixels?\n",
        "    smooth = True\n",
        "    \n",
        "    # Interpolation type.\n",
        "    if smooth:\n",
        "        interpolation = 'sinc'\n",
        "    else:\n",
        "        interpolation = 'nearest'\n",
        "\n",
        "    # Plot the content-image.\n",
        "    # Note that the pixel-values are normalized to\n",
        "    # the [0.0, 1.0] range by dividing with 255.\n",
        "    ax = axes.flat[0]\n",
        "    ax.imshow(content_image / 255.0, interpolation=interpolation)\n",
        "    ax.set_xlabel(\"Content\")\n",
        "\n",
        "    # Plot the mixed-image.\n",
        "    ax = axes.flat[1]\n",
        "    ax.imshow(mixed_image / 255.0, interpolation=interpolation)\n",
        "    ax.set_xlabel(\"Mixed\")\n",
        "\n",
        "    # Plot the style-image\n",
        "    ax = axes.flat[2]\n",
        "    ax.imshow(style_image / 255.0, interpolation=interpolation)\n",
        "    ax.set_xlabel(\"Style\")\n",
        "\n",
        "    # Remove ticks from all the plots.\n",
        "    for ax in axes.flat:\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "    \n",
        "    # Ensure the plot is shown correctly with multiple plots\n",
        "    # in a single Notebook cell.\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "st1nlLC3t5OS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#average of the squre of the errors. \n",
        "#between output values for both images at a given layer\n",
        "#The smaller the means squared error, the closer you are to finding the line of best fit. \n",
        "# square the difference between both output feature map/activation map\n",
        "# and average all those values by adding all then dividing by n \n",
        "# https://www.tensorflow.org/api_docs/python/tf/reduce_mean\n",
        "def mean_squared_error(a, b):\n",
        "    return tf.reduce_mean(tf.square(a - b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "goIxOr5ot8bq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#step 3 - content image is 3d numpy array, indices for the layers\n",
        "#we want to use for content loss\n",
        "#you should expirment what looks good for different layers\n",
        "#there is not one best layer, we haven't found a way to minimize\n",
        "#loss for beauty. how to quantify?\n",
        "def create_content_loss(session, model, content_image, layer_ids):\n",
        "    \"\"\"\n",
        "    Create the loss-function for the content-image.\n",
        "    \n",
        "    Parameters:\n",
        "    session: An open TensorFlow session for running the model's graph.\n",
        "    model: The model, e.g. an instance of the VGG16-class.\n",
        "    content_image: Numpy float array with the content-image.\n",
        "    layer_ids: List of integer id's for the layers to use in the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    #A python dictionary object is generated with the \n",
        "    # placeholders as keys and the representative feed \n",
        "    #tensors as values.\n",
        "    # Create a feed-dict with the content-image.\n",
        "    feed_dict = model.create_feed_dict(image=content_image)\n",
        "\n",
        "    # Get references to the tensors for the given layers.\n",
        "    # collection of filters\n",
        "    layers = model.get_layer_tensors(layer_ids)\n",
        "\n",
        "    # Calculate the output values of those layers when\n",
        "    # feeding the content-image to the model.\n",
        "    values = session.run(layers, feed_dict=feed_dict)\n",
        "\n",
        "    # Set the model's graph as the default so we can add\n",
        "    # computational nodes to it. It is not always clear\n",
        "    # when this is necessary in TensorFlow, but if you\n",
        "    # want to re-use this code then it may be necessary.\n",
        "    with model.graph.as_default():\n",
        "        # Initialize an empty list of loss-functions.\n",
        "        #because we are calculating losses per layer\n",
        "        layer_losses = []\n",
        "    \n",
        "        # For each layer and its corresponding values\n",
        "        # for the content-image.\n",
        "        for value, layer in zip(values, layers):\n",
        "            # These are the values that are calculated\n",
        "            # for this layer in the model when inputting\n",
        "            # the content-image. Wrap it to ensure it\n",
        "            # is a const - although this may be done\n",
        "            # automatically by TensorFlow.\n",
        "            value_const = tf.constant(value)\n",
        "\n",
        "            # The loss-function for this layer is the\n",
        "            # Mean Squared Error between the layer-values\n",
        "            # when inputting the content- and mixed-images.\n",
        "            # Note that the mixed-image is not calculated\n",
        "            # yet, we are merely creating the operations\n",
        "            # for calculating the MSE between those two.\n",
        "            loss = mean_squared_error(layer, value_const)\n",
        "\n",
        "            # Add the loss-function for this layer to the\n",
        "            # list of loss-functions.\n",
        "            layer_losses.append(loss)\n",
        "\n",
        "        # The combined loss for all layers is just the average.\n",
        "        # The loss-functions could be weighted differently for\n",
        "        # each layer. You can try it and see what happens.\n",
        "        total_loss = tf.reduce_mean(layer_losses)\n",
        "        \n",
        "    return total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tJ-3EQVYt_P4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#The Gram matrix, defined as https://i.stack.imgur.com/tU9ow.png \n",
        "#is used to \n",
        "#measure the correlation between channels after flattening the \n",
        "#filter images into vectors\n",
        "\n",
        "#Gatys when asked why gram matrix at a talk was that the \n",
        "#Gram matrix encodes second \n",
        "#order statistics of the set of filters.\n",
        "#it sort of mushes up all the features at a given layer, \n",
        "#tossing spatial information in favor of a measure of \n",
        "#how the different features are correlated \n",
        "\n",
        "def gram_matrix(tensor):\n",
        "    #gram matrix is vector of dot products for vectors\n",
        "    #of the feature activations of a style layer\n",
        "    \n",
        "    #4d tensor from convolutional layer\n",
        "    shape = tensor.get_shape()\n",
        "    \n",
        "    # Get the number of feature channels for the input tensor,\n",
        "    # which is assumed to be from a convolutional layer with 4-dim.\n",
        "    num_channels = int(shape[3])\n",
        "\n",
        "    #-1 means whatever number makes the data fit \n",
        "    # Reshape the tensor so it is a 2-dim matrix. This essentially\n",
        "    # flattens the contents of each feature-channel.\n",
        "    matrix = tf.reshape(tensor, shape=[-1, num_channels])\n",
        "    \n",
        "    \n",
        "    #gram matrix is transpose matrix with itself\n",
        "    #so each entry in gram matrix\n",
        "    #tells us if a feature channel has a tendency\n",
        "    #to be activated with another feature channel\n",
        "    \n",
        "    #idea is to make the mixed image match patterns from style image\n",
        "    \n",
        "    \n",
        "    # Calculate the Gram-matrix as the matrix-product of\n",
        "    # the 2-dim matrix with itself. This calculates the\n",
        "    # dot-products of all combinations of the feature-channels.\n",
        "    gram = tf.matmul(tf.transpose(matrix), matrix)\n",
        "\n",
        "    return gram                            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cFgzoIPIuCo1",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def create_style_loss(session, model, style_image, layer_ids):\n",
        "    \"\"\"\n",
        "    Create the loss-function for the style-image.\n",
        "    \n",
        "    Parameters:\n",
        "    session: An open TensorFlow session for running the model's graph.\n",
        "    model: The model, e.g. an instance of the VGG16-class.\n",
        "    style_image: Numpy float array with the style-image.\n",
        "    layer_ids: List of integer id's for the layers to use in the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a feed-dict with the style-image.\n",
        "    feed_dict = model.create_feed_dict(image=style_image)\n",
        "\n",
        "    # Get references to the tensors for the given layers.\n",
        "    layers = model.get_layer_tensors(layer_ids)\n",
        "\n",
        "    # Set the model's graph as the default so we can add\n",
        "    # computational nodes to it. It is not always clear\n",
        "    # when this is necessary in TensorFlow, but if you\n",
        "    # want to re-use this code then it may be necessary.\n",
        "    with model.graph.as_default():\n",
        "        # Construct the TensorFlow-operations for calculating\n",
        "        # the Gram-matrices for each of the layers.\n",
        "        gram_layers = [gram_matrix(layer) for layer in layers]\n",
        "\n",
        "        # Calculate the values of those Gram-matrices when\n",
        "        # feeding the style-image to the model.\n",
        "        values = session.run(gram_layers, feed_dict=feed_dict)\n",
        "\n",
        "        # Initialize an empty list of loss-functions.\n",
        "        layer_losses = []\n",
        "    \n",
        "        # For each Gram-matrix layer and its corresponding values.\n",
        "        for value, gram_layer in zip(values, gram_layers):\n",
        "            # These are the Gram-matrix values that are calculated\n",
        "            # for this layer in the model when inputting the\n",
        "            # style-image. Wrap it to ensure it is a const,\n",
        "            # although this may be done automatically by TensorFlow.\n",
        "            value_const = tf.constant(value)\n",
        "\n",
        "            # The loss-function for this layer is the\n",
        "            # Mean Squared Error between the Gram-matrix values\n",
        "            # for the content- and mixed-images.\n",
        "            # Note that the mixed-image is not calculated\n",
        "            # yet, we are merely creating the operations\n",
        "            # for calculating the MSE between those two.\n",
        "            \n",
        "            \n",
        "            #between gram matrix and value of gram matrix when inputting \n",
        "            #style image\n",
        "            \n",
        "            loss = mean_squared_error(gram_layer, value_const)\n",
        "\n",
        "            # Add the loss-function for this layer to the\n",
        "            # list of loss-functions.\n",
        "            layer_losses.append(loss)\n",
        "\n",
        "        # The combined loss for all layers is just the average.\n",
        "        # The loss-functions could be weighted differently for\n",
        "        # each layer. You can try it and see what happens.\n",
        "        total_loss = tf.reduce_mean(layer_losses)\n",
        "        \n",
        "    return total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ENAy69fwuFrG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#shifts input image by 1 pixel on x and y axis \n",
        "#calculate difference between shifted and original image\n",
        "#absolute value to make positive\n",
        "#calculate sum of pixels in those images\n",
        "#helps suppress noise in mixed image we are generating\n",
        "\n",
        "\n",
        "\n",
        "def create_denoise_loss(model):\n",
        "    loss = tf.reduce_sum(tf.abs(model.input[:,1:,:,:] - model.input[:,:-1,:,:])) + \\\n",
        "           tf.reduce_sum(tf.abs(model.input[:,:,1:,:] - model.input[:,:,:-1,:]))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iswcy3dXuIoQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#relative weights - normalized.\n",
        "#style should have more weight than content\n",
        "def style_transfer(content_image, style_image,\n",
        "                   content_layer_ids, style_layer_ids,\n",
        "                   weight_content=1.5, weight_style=10.0,\n",
        "                   weight_denoise=0.3,\n",
        "                   num_iterations=1, step_size=10.0):\n",
        "    \"\"\"\n",
        "    Use gradient descent to find an image that minimizes the\n",
        "    loss-functions of the content-layers and style-layers. This\n",
        "    should result in a mixed-image that resembles the contours\n",
        "    of the content-image, and resembles the colours and textures\n",
        "    of the style-image.\n",
        "    \n",
        "    Parameters:\n",
        "    content_image: Numpy 3-dim float-array with the content-image.\n",
        "    style_image: Numpy 3-dim float-array with the style-image.\n",
        "    content_layer_ids: List of integers identifying the content-layers.\n",
        "    style_layer_ids: List of integers identifying the style-layers.\n",
        "    weight_content: Weight for the content-loss-function.\n",
        "    weight_style: Weight for the style-loss-function.\n",
        "    weight_denoise: Weight for the denoising-loss-function.\n",
        "    num_iterations: Number of optimization iterations to perform.\n",
        "    step_size: Step-size for the gradient in each iteration.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create an instance of the VGG16-model. This is done\n",
        "    # in each call of this function, because we will add\n",
        "    # operations to the graph so it can grow very large\n",
        "    # and run out of RAM if we keep using the same instance.\n",
        "    model = VGG16()\n",
        "\n",
        "    # Create a TensorFlow-session.\n",
        "    session = tf.InteractiveSession(graph=model.graph)\n",
        "\n",
        "    # Print the names of the content-layers.\n",
        "    print(\"Content layers:\")\n",
        "    print(model.get_layer_names(content_layer_ids))\n",
        "    print()\n",
        "\n",
        "    # Print the names of the style-layers.\n",
        "    print(\"Style layers:\")\n",
        "    print(model.get_layer_names(style_layer_ids))\n",
        "    print()\n",
        "\n",
        "    # Create the loss-function for the content-layers and -image.\n",
        "    loss_content = create_content_loss(session=session,\n",
        "                                       model=model,\n",
        "                                       content_image=content_image,\n",
        "                                       layer_ids=content_layer_ids)\n",
        "\n",
        "    # Create the loss-function for the style-layers and -image.\n",
        "    loss_style = create_style_loss(session=session,\n",
        "                                   model=model,\n",
        "                                   style_image=style_image,\n",
        "                                   layer_ids=style_layer_ids)    \n",
        "\n",
        "    # Create the loss-function for the denoising of the mixed-image.\n",
        "    loss_denoise = create_denoise_loss(model)\n",
        "\n",
        "    \n",
        "    #adjust levels of loss functions, normalize them\n",
        "    #multiply them with a variable\n",
        "    #taking reciprocal values of loss values of content, style, denoising\n",
        "    #small constant to avoid divide by 0\n",
        "    #adjustment value normalizes loss so approximately 1\n",
        "    #weights should be set relative to each other dont depend on layers\n",
        "    #we are using\n",
        "    \n",
        "    # Create TensorFlow variables for adjusting the values of\n",
        "    # the loss-functions. This is explained below.\n",
        "    adj_content = tf.Variable(1e-10, name='adj_content')\n",
        "    adj_style = tf.Variable(1e-10, name='adj_style')\n",
        "    adj_denoise = tf.Variable(1e-10, name='adj_denoise')\n",
        "\n",
        "    # Initialize the adjustment values for the loss-functions.\n",
        "    session.run([adj_content.initializer,\n",
        "                 adj_style.initializer,\n",
        "                 adj_denoise.initializer])\n",
        "\n",
        "    # Create TensorFlow operations for updating the adjustment values.\n",
        "    # These are basically just the reciprocal values of the\n",
        "    # loss-functions, with a small value 1e-10 added to avoid the\n",
        "    # possibility of division by zero.\n",
        "    update_adj_content = adj_content.assign(1.0 / (loss_content + 1e-10))\n",
        "    update_adj_style = adj_style.assign(1.0 / (loss_style + 1e-10))\n",
        "    update_adj_denoise = adj_denoise.assign(1.0 / (loss_denoise + 1e-10))\n",
        "\n",
        "    # This is the weighted loss-function that we will minimize\n",
        "    # below in order to generate the mixed-image.\n",
        "    # Because we multiply the loss-values with their reciprocal\n",
        "    # adjustment values, we can use relative weights for the\n",
        "    # loss-functions that are easier to select, as they are\n",
        "    # independent of the exact choice of style- and content-layers.\n",
        "    loss_combined = weight_content * adj_content * loss_content + \\\n",
        "                    weight_style * adj_style * loss_style + \\\n",
        "                    weight_denoise * adj_denoise * loss_denoise\n",
        "\n",
        "    # Use TensorFlow to get the mathematical function for the\n",
        "    # gradient of the combined loss-function with regard to\n",
        "    # the input image. (mixed)\n",
        "    gradient = tf.gradients(loss_combined, model.input)\n",
        "\n",
        "    # List of tensors that we will run in each optimization iteration.\n",
        "    run_list = [gradient, update_adj_content, update_adj_style, \\\n",
        "                update_adj_denoise]\n",
        "\n",
        "    # The mixed-image is initialized with random noise.\n",
        "    # It is the same size as the content-image.\n",
        "    #where we first init it\n",
        "    mixed_image = np.random.rand(*content_image.shape) + 128\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Create a feed-dict with the mixed-image.\n",
        "        feed_dict = model.create_feed_dict(image=mixed_image)\n",
        "\n",
        "        # Use TensorFlow to calculate the value of the\n",
        "        # gradient, as well as updating the adjustment values.\n",
        "        grad, adj_content_val, adj_style_val, adj_denoise_val \\\n",
        "        = session.run(run_list, feed_dict=feed_dict)\n",
        "\n",
        "        # Reduce the dimensionality of the gradient.\n",
        "        #Remove single-dimensional entries from the shape of an array.\n",
        "        grad = np.squeeze(grad)\n",
        "\n",
        "        # Scale the step-size according to the gradient-values.\n",
        "        #Ratio of weights:updates\n",
        "        #akin to learning rate\n",
        "        step_size_scaled = step_size / (np.std(grad) + 1e-8)\n",
        "\n",
        "        # Update the image by following the gradient.\n",
        "        #gradient descent\n",
        "        mixed_image -= grad * step_size_scaled\n",
        "\n",
        "        # Ensure the image has valid pixel-values between 0 and 255.\n",
        "        #Given an interval, values outside the interval are clipped \n",
        "        #to the interval edges.\n",
        "        mixed_image = np.clip(mixed_image, 0.0, 255.0)\n",
        "\n",
        "        # Print a little progress-indicator.\n",
        "        print(\". \", end=\"\")\n",
        "\n",
        "        # Display status once every 10 iterations, and the last.\n",
        "        if (i % 10 == 0) or (i == num_iterations - 1):\n",
        "            print()\n",
        "            print(\"Iteration:\", i)\n",
        "\n",
        "            # Print adjustment weights for loss-functions.\n",
        "            msg = \"Weight Adj. for Content: {0:.2e}, Style: {1:.2e}, Denoise: {2:.2e}\"\n",
        "            print(msg.format(adj_content_val, adj_style_val, adj_denoise_val))\n",
        "\n",
        "            #in larger resolution\n",
        "            # Plot the content-, style- and mixed-images.\n",
        "            plot_images(content_image=content_image,\n",
        "                        style_image=style_image,\n",
        "                        mixed_image=mixed_image)\n",
        "            \n",
        "    print()\n",
        "    print(\"Final image:\")\n",
        "    plot_image_big(mixed_image)\n",
        "\n",
        "    # Close the TensorFlow session to release its resources.\n",
        "    session.close()\n",
        "    \n",
        "    # Return the mixed-image.\n",
        "    return mixed_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I6PDf9eUuLqz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "content_filename = 'How-to-Generate-Art-Demo/images/imagenet-sample.jpg'\n",
        "content_image = load_image(content_filename, max_size=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1nOmZuCDuPb5",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "style_filename = 'How-to-Generate-Art-Demo/images/styles/starry_night.jpg'\n",
        "style_image = load_image(style_filename, max_size=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R_OoGTJFuSGb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "content_layer_ids = [4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JptuNw8_uVkE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# The VGG16-model has 13 convolutional layers.\n",
        "# This selects all those layers as the style-layers.\n",
        "# This is somewhat slow to optimize.\n",
        "style_layer_ids = list(range(13))\n",
        "\n",
        "# You can also select a sub-set of the layers, e.g. like this:\n",
        "# style_layer_ids = [1, 2, 3, 4]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pnQHCcjiuYkX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "bef87cc2-d50c-46d0-fb4d-49a486df38c7"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "img = style_transfer(content_image=content_image,\n",
        "                     style_image=style_image,\n",
        "                     content_layer_ids=content_layer_ids,\n",
        "                     style_layer_ids=style_layer_ids,\n",
        "                     weight_content=1.5,\n",
        "                     weight_style=10.0,\n",
        "                     weight_denoise=0.3,\n",
        "                     num_iterations=100,\n",
        "                     step_size=10.0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Content layers:\n",
            "['conv3_1/conv3_1']\n",
            "\n",
            "Style layers:\n",
            "['conv1_1/conv1_1', 'conv1_2/conv1_2', 'conv2_1/conv2_1', 'conv2_2/conv2_2', 'conv3_1/conv3_1', 'conv3_2/conv3_2', 'conv3_3/conv3_3', 'conv4_1/conv4_1', 'conv4_2/conv4_2', 'conv4_3/conv4_3', 'conv5_1/conv5_1', 'conv5_2/conv5_2', 'conv5_3/conv5_3']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vctEKKCxubhg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}